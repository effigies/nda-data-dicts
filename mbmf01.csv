"ElementName","DataType","Size","Required","ElementDescription","ValueRange","Notes","Aliases"
"subjectkey","GUID","","Required","The NDAR Global Unique Identifier (GUID) for research subject","NDAR*","",""
"src_subject_id","String","20","Required","Subject ID how it's defined in lab/project","","",""
"interview_date","Date","","Required","Date on which the interview/genetic test/sampling/imaging/biospecimen was completed. MM/DD/YYYY","","",""
"interview_age","Integer","","Required","Age in months at the time of the interview/test/sampling/imaging.","0::1440","Age is rounded to chronological month. If the research participant is 15-days-old at time of interview, the appropriate value would be 0 months. If the participant is 16-days-old, the value would be 1 month.",""
"sex","String","20","Required","Sex of subject at birth","M;F; O; NR","M = Male; F = Female; O=Other; NR = Not reported",""
"experiment_id","Integer","","Recommended","ID for the Experiment/settings/run","","",""
"trialid","Integer","","Recommended","Trial number within entire task","","",""
"condstate","Integer","","Recommended","Current Transition Reliability condition","1;2","1 = High Uncertainty (transition probabilities 0.9, 0.1); 2 = Low Uncertainty (transition probabilities 0.5, 0.5)",""
"condreward","Integer","","Recommended","Current Reward Reliability condition","1;2","1 = High Uncertainty (reward magnitude between 0.3, 1.0); 2 = Low Uncertainty (reward magnitude between 0.1, 0.19)",""
"doraretrans","Integer","","Recommended","Indicates whether transition will be rare","0;1","0 = common transition; 1 = rare transition",""
"rewpwin_1","Float","","Recommended","Probability trial is rewarded in case final state ID is 1","0::1","",""
"rewpwin_2","Float","","Recommended","Probability trial is rewarded in case final state ID is 2","0::1","",""
"rewpwin_3","Float","","Recommended","Probability trial is rewarded in case final state ID is 3","0::1","",""
"rewpwin_4","Float","","Recommended","Probability trial is rewarded in case final state ID is 4","0::1","",""
"rewbinary_1","Integer","","Recommended","Indicates whether trial is rewarded in case final state ID is 1","0;1","0 = No; 1 = Yes",""
"rewbinary_2","Integer","","Recommended","Indicates whether trial is rewarded in case final state ID is 2","0;1","0 = No; 1 = Yes",""
"rewbinary_3","Integer","","Recommended","Indicates whether trial is rewarded in case final state ID is 3","0;1","0 = No; 1 = Yes",""
"rewbinary_4","Integer","","Recommended","Indicates whether trial is rewarded in case final state ID is 4","0;1","0 = No; 1 = Yes",""
"state2_1","Integer","","Recommended","ID of final state if subject chooses Left stimulus (yellow ship)","1;2;3;4","1 = state 1; 2 = state 2; 3 = state 3; 4 = state 4",""
"state2_2","Integer","","Recommended","ID of final state if subject chooses Right stimulus (blue ship)","1;2;3;4","1 = state 1; 2 = state 2; 3 = state 3; 4 = state 4",""
"rewmagnitude","Float","","Recommended","Magnitude of reward if subject is rewarded on current trial, divided by 100","","",""
"outcome_01","Integer","","Recommended","ID of final state resulting from subject's chosen action","1::4;-999","1 = state 1; 2 = state 2; 3 = state 3; 4 = state 4; -999 indicates missed response (subject took longer than 2s)","outcome1"
"outcomebin","Integer","","Recommended","Indicates whether subject choice is rewarded or not","0;1;-999","0 = Not rewarded; 1 = Rewarded; -999 = Missing",""
"outcomemag","Integer","","Recommended","Magnitude of actual reward (if any) on current trial","","A value of -999 indicates missed response (subject took longer than 2s)",""
"resp_02","Integer","","Recommended","Model Based Model Free Task - Subject's response","1;2;-999","1 = Left; 2 = Right; -999 indicates missed response (subject took longer than 2s)","resp1"
"rt_01","Float","","Recommended","Time elapsed between stimulus onset and subject reponse","","In milliseconds. -999 indicates missed response (subject took longer than 2s)","rt1"
"practice1","Integer","","Recommended","Is the trial a practice trial?","0;1","0 = main task; 1 = practice","practice"
"condcont","Integer","","Recommended","Current Reward Contingency condition","1;2","1 = Stimulus-Contingent Reward (reward probability curves map to left/right stimulus); 2 = State-Contingent Reward (reward probability curves map to final state)",""
"condstate2","Integer","","Recommended","Current Transition Reliability condition","1;2","1 = Low Reliability (transition probabilities 0.5, 0.5); 2 = High Reliability (transition probabilities 0.9, 0.1)",""
"condreward2","Integer","","Recommended","Current Reward Reliability condition","1;2","1 = Low Reliability (reward magnitude between 0.3, 1.0); 2 = High Reliability (reward magnitude between 0.1, 0.19)",""
"food_money_outcome","Integer","","Recommended","Food or Money outcomes","1;2","1=Money; 2=Food",""
"mf_parameter","Float","","Recommended","Model-free learning parameter estimated from computational models ","0 :: 1.0","",""
"mb_parameter","Float","","Recommended","Model-based learning parameter estimated from computational models ","0 :: 1.0","",""
"habit_probe","Float","","Recommended","Difference score of choose_best versus avoid_worst in the Probe phase ","0 :: 1.0","",""
"choose_best","Float","","Recommended","Proportion trials the most rewarding outcome is chosen in a 2AFC task across all trials in probe phase","0 :: 1.0","",""
"avoid_worst","Float","","Recommended","Proportion trials the least rewarding outcome is avoided in a 2AFC task across all trials in probe phase","0 :: 1.0","",""
